{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Item Demand Forecasting"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The objective of this project is to predict 3 months of item-level sales data at different store locations.\n",
    "We are given 5 years of store-item sales data, and asked to predict 3 months of sales for 50 different items at 10 different stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_excel('train.xlsx')\n",
    "train['date'] = pd.to_datetime(train['date'], format=\"%Y-%m-%d\")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "no missing values in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many stores and items are there?\n",
    "train.store.nunique(), train.item.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Range\n",
    "train[\"date\"].min(), train[\"date\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many items are in the store?\n",
    "train.groupby([\"store\"])[\"item\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Stats for each store\n",
    "train.groupby([\"store\"]).agg({\"sales\": [\"count\",\"sum\", \"mean\", \"median\", \"std\", \"min\", \"max\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Stats for each item\n",
    "train.groupby([\"item\"]).agg({\"sales\": [\"count\",\"sum\", \"mean\", \"median\", \"std\", \"min\", \"max\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Histogram for Store Sales\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 10))\n",
    "for i in range(1,11):\n",
    "    if i < 6:\n",
    "        train[train.store == i].sales.hist(ax=axes[0, i-1])\n",
    "        axes[0,i-1].set_title(\"Store \" + str(i), fontsize = 15)\n",
    "        \n",
    "    else:\n",
    "        train[train.store == i].sales.hist(ax=axes[1, i - 6])\n",
    "        axes[1,i-6].set_title(\"Store \" + str(i), fontsize = 15)\n",
    "plt.tight_layout(pad=4.5)\n",
    "plt.suptitle(\"Histogram: Sales\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['sales'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check for outliers\n",
    "fig=plt.subplots(figsize=(12,2))\n",
    "ax=sns.boxplot(x=train['sales'],whis=1.5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Therefore Sales follow a Positively Skewed Distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation between total sales of stores\n",
    "\n",
    "storesales = train.groupby([\"date\", \"store\"]).sales.sum().reset_index().set_index(\"date\")\n",
    "corr =  pd.pivot_table(storesales, values = \"sales\", columns=\"store\", index=\"date\").corr(method = \"spearman\")\n",
    "plt.figure(figsize = (7,7))\n",
    "sns.heatmap(corr[(corr >= 0.5) | (corr <= -0.5)], \n",
    "            cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1,\n",
    "            annot=True, annot_kws={\"size\": 9}, square=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig=px.line(train, x='date', y='sales', title='Sales with Slider')\n",
    "\n",
    "fig.update_xaxes(rangeslider_visible=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales distribution across the train data\n",
    "def sales_dist(data):\n",
    "    \"\"\"\n",
    "        Sales_dist used for Checking Sales Distribution.\n",
    "        data :  contain data frame which contain sales data\n",
    "    \"\"\"\n",
    "    sales_df = data.copy(deep=True)\n",
    "    sales_df['sales_bins'] = pd.cut(sales_df.sales, [0, 50, 100, 150, 200, 250])\n",
    "    print('Max sale:', sales_df.sales.max())\n",
    "    print('Min sale:', sales_df.sales.min())\n",
    "    print('Avg sale:', sales_df.sales.mean())\n",
    "    print()\n",
    "    return sales_df\n",
    "\n",
    "sales_df = sales_dist(train)\n",
    "\n",
    "# Total number of data points\n",
    "total_points = pd.value_counts(sales_df.sales_bins).sum()\n",
    "print('Sales bucket v/s Total percentage:')\n",
    "display(pd.value_counts(sales_df.sales_bins).apply(lambda s: (s/total_points)*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To understand the sales data distribution across the stores\n",
    "def sales_data_understanding(data):    \n",
    "    store_df = data.copy()\n",
    "    plt.figure(figsize=(20,10))\n",
    "    sales_pivoted_df = pd.pivot_table(store_df, index='store', values=['sales','date'], columns='item', aggfunc=np.mean)\n",
    "    #sales_pivoted_df.plot(kind=\"hist\",figsize=(20,10))\n",
    "    # Pivoted dataframe\n",
    "    display(sales_pivoted_df)\n",
    "    return (store_df,sales_pivoted_df)\n",
    "\n",
    "store_df,sales_pivoted_df = sales_data_understanding(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average sales of all the items by each store\n",
    "sales_across_store_df = sales_pivoted_df.copy()\n",
    "sales_across_store_df['avg_sale'] = sales_across_store_df.apply(lambda r: r.mean(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "# Scatter plot of average sales per store\n",
    "\n",
    "sales_store_data = go.Scatter(\n",
    "    y = sales_across_store_df.avg_sale.values,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size = sales_across_store_df.avg_sale.values,\n",
    "        color = sales_across_store_df.avg_sale.values,\n",
    "        colorscale='Viridis',\n",
    "        showscale=True\n",
    "    ),\n",
    "    text = sales_across_store_df.index.values\n",
    ")\n",
    "data = [sales_store_data]\n",
    "\n",
    "sales_store_layout = go.Layout(\n",
    "    autosize= True,\n",
    "    title= 'Scatter plot of avg sales per store',\n",
    "    hovermode= 'closest',\n",
    "    xaxis= dict(\n",
    "         title= 'Stores',\n",
    "        ticklen= 10,\n",
    "        zeroline= False,\n",
    "        gridwidth= 1,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'Avg Sales',\n",
    "        ticklen= 10,\n",
    "        zeroline= False,\n",
    "        gridwidth= 1,\n",
    "    ),\n",
    "    showlegend= False\n",
    ")\n",
    "fig = go.Figure(data=data, layout=sales_store_layout)\n",
    "py.iplot(fig,filename='scatter_sales_store')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "From the visualization, it is clear that the stores with ID 2 and 8 have higher average sales than the remaining stores and is a clear indication that they are doing good money whereas store with ID 7 has very poor performance in terms of average sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does sales vary across items\n",
    "\n",
    "def sales_insight(sales_pivoted_df):\n",
    "    # Let us calculate the average sales of each of the item across all the stores\n",
    "    sales_across_item_df = sales_pivoted_df.copy()\n",
    "    # Aggregate the sales per item and add it as a new row in the same dataframe\n",
    "    sales_across_item_df.loc[11] = sales_across_item_df.apply(lambda r: r.mean(), axis=0)\n",
    "    # Note the 11th index row, which is the average sale of each of the item across all the stores\n",
    "    #display(sales_across_item_df.loc[11:])\n",
    "    avg_sales_per_item_across_stores_df = pd.DataFrame(data=[[i+1,a] for i,a in enumerate(sales_across_item_df.loc[11:].values[0])], columns=['item', 'avg_sale'])\n",
    "    # And finally, sort by avg sale\n",
    "    avg_sales_per_item_across_stores_df.sort_values(by='avg_sale', ascending=False, inplace=True)\n",
    "    \n",
    "    # Display the top 5 rows\n",
    "    display(avg_sales_per_item_across_stores_df.head())\n",
    "    return (sales_across_item_df,avg_sales_per_item_across_stores_df)\n",
    "\n",
    "sales_across_item_df,avg_sales_per_item_across_stores_df = sales_insight(sales_pivoted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sales_per_item_across_stores_sorted = avg_sales_per_item_across_stores_df.avg_sale.values\n",
    "# Scatter plot of average sales per item\n",
    "sales_item_data = go.Bar(\n",
    "    x=[i for i in range(0, 50)],\n",
    "    y=avg_sales_per_item_across_stores_sorted,\n",
    "    marker=dict(\n",
    "        color=avg_sales_per_item_across_stores_sorted,\n",
    "        colorscale='Blackbody',\n",
    "        showscale=True\n",
    "    ),\n",
    "    text = avg_sales_per_item_across_stores_df.item.values\n",
    ")\n",
    "data = [sales_item_data]\n",
    "\n",
    "sales_item_layout = go.Layout(\n",
    "    autosize= True,\n",
    "    title= 'Scatter plot of avg sales per item',\n",
    "    hovermode= 'closest',\n",
    "    xaxis= dict(\n",
    "        title= 'Items',\n",
    "        ticklen= 55,\n",
    "        zeroline= False,\n",
    "        gridwidth= 1,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title= 'Avg Sales',\n",
    "        ticklen= 10,\n",
    "        zeroline= False,\n",
    "        gridwidth= 1,\n",
    "    ),\n",
    "    showlegend= False\n",
    ")\n",
    "fig = go.Figure(data=data, layout=sales_item_layout)\n",
    "py.iplot(fig,filename='scatter_sales_item')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Top items with highest average sale are 15, 28, 13, 18 and with least average sales are 5, 1, 41 and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Time_visualization(data):\n",
    "    store_item_df = data.copy()\n",
    "    # First, let us filterout the required data\n",
    "    store_id = 10   # Some store\n",
    "    item_id = 40    # Some item\n",
    "    print('Before filter:', store_item_df.shape)\n",
    "    store_item_df = store_item_df[store_item_df.store == store_id]\n",
    "    store_item_df = store_item_df[store_item_df.item == item_id]\n",
    "    print('After filter:', store_item_df.shape)\n",
    "    #display(store_item_df.head())\n",
    "\n",
    "    # Let us plot this now\n",
    "    store_item_ts_data = [go.Scatter(\n",
    "        x=store_item_df.date,\n",
    "        y=store_item_df.sales)]\n",
    "    py.iplot(store_item_ts_data)\n",
    "    return store_item_df\n",
    "\n",
    "store_item_df = Time_visualization(train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The sales go high in June, July and August months. The sales will be lowest in December, January and February months. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sales_monthly(data):\n",
    "    multi_store_item_df = data.copy()\n",
    "    # First, let us filterout the required data\n",
    "    store_ids = [1, 1, 1, 1]   # Some stores\n",
    "    item_ids = [10, 20, 30, 40]    # Some items\n",
    "    print('Before filter:', multi_store_item_df.shape)\n",
    "    multi_store_item_df = multi_store_item_df[multi_store_item_df.store.isin(store_ids)]\n",
    "    multi_store_item_df = multi_store_item_df[multi_store_item_df.item.isin(item_ids)]\n",
    "    print('After filter:', multi_store_item_df.shape)\n",
    "    #display(multi_store_item_df)\n",
    "    # TODO Monthly avg sales\n",
    "    \n",
    "    # Let us plot this now\n",
    "    multi_store_item_ts_data = []\n",
    "    for st,it in zip(store_ids, item_ids):\n",
    "        flt = multi_store_item_df[multi_store_item_df.store == st]\n",
    "        flt = flt[flt.item == it]\n",
    "        multi_store_item_ts_data.append(go.Scatter(x=flt.date, y=flt.sales, name = \"Store:\" + str(st) + \",Item:\" + str(it)))\n",
    "    py.iplot(multi_store_item_ts_data)\n",
    "    return (multi_store_item_df)\n",
    "\n",
    "multi_store_item_df = sales_monthly(train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Though the pattern remains same across different stores and items combinations, the actual sale value consitently varies with the same scale.\n",
    "\n",
    "As we can see in the visualization, item 10 has consistently highest sales through out the span of 5 years!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per 1 store, 1 item\n",
    "train_df = train[train['store']==1]\n",
    "train_df = train_df[train['item']==1]\n",
    "# train_df = train_df.set_index('date')\n",
    "train_df['year'] = train['date'].dt.year\n",
    "train_df['month'] = train['date'].dt.month\n",
    "train_df['day'] = train['date'].dt.dayofyear\n",
    "train_df['weekday'] = train['date'].dt.weekday\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decompose the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"date\", y=\"sales\",legend = 'full' , data=train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=\"date\", y=\"sales\",legend = 'full' , data=train_df[:28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"weekday\", y=\"sales\", data=train_df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Monday=0, Sunday=6\n",
    "Here it can be seen the weekends(5,6) has a larger sales, weekdays(0-4) are smaller. There's a few outliers on Monday, Wed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.set_index('date')\n",
    "train_df['sales'] = train_df['sales'].astype(float)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "result = seasonal_decompose(train_df['sales'], model='additive', freq=365)\n",
    "\n",
    "fig = plt.figure()  \n",
    "fig = result.plot()  \n",
    "fig.set_size_inches(15, 12)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "There is an yearly pattern in the data and also we can see a upwards trend. \n",
    "This means this data is not stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "def test_stationarity(timeseries, window = 365, cutoff = 0.01):\n",
    "\n",
    "    #Determing rolling statistics\n",
    "    rolmean = timeseries.rolling(window).mean()\n",
    "    rolstd = timeseries.rolling(window).std()\n",
    "\n",
    "    #Plot rolling statistics:\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    orig = plt.plot(timeseries, color='blue',label='Original')\n",
    "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show()\n",
    "    \n",
    "    #Perform Dickey-Fuller test:\n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC', maxlag = 20 )\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    pvalue = dftest[1]\n",
    "    if pvalue < cutoff:\n",
    "        print('p-value = %.4f. The series is likely stationary.' % pvalue)\n",
    "    else:\n",
    "        print('p-value = %.4f. The series is likely non-stationary.' % pvalue)\n",
    "    \n",
    "    print(dfoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stationarity(train_df['sales'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "H0: The data is not stationary           H1: The data is stationary \n",
    "D.C. Reject H0 if p value <= alpha\n",
    "\n",
    "Here the p-value is 0.036. If 5% Critical Value(CV) is used, this series would be considered stationary. But as just visually  upward trend was found, we want to be more strict, we use 1% CV.\n",
    "\n",
    "To get a stationary data, there's many techiniques. We can use log, differencing etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_diff = train_df.sales - train_df.sales.shift(1)\n",
    "first_diff = first_diff.dropna(inplace = False)\n",
    "test_stationarity(first_diff, window = 365)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "After differencing, the p-value is extremely small. Thus now this series is very likely to be stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check for outliers for store 1, item 1\n",
    "fig=plt.subplots(figsize=(12,2))\n",
    "ax=sns.boxplot(x=train_df['sales'],whis=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACF and PACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "fig = sm.graphics.tsa.plot_acf(first_diff, lags=40, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "fig = sm.graphics.tsa.plot_pacf(first_diff, lags=40, ax=ax2)\n",
    "\n",
    "# Here we can see the acf and pacf both has a recurring pattern every 7 periods. Indicating a weekly pattern exists. \n",
    "# Any time you see a regular pattern like that in one of these plots, implies that there is some sort of \n",
    "# significant seasonal thing going on. Then we should also consider SARIMA to take seasonality into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_mod6 = sm.tsa.ARIMA(train_df.sales, (14,1,1)).fit(disp=False)\n",
    "print(arima_mod6.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 1730\n",
    "end_index = 1825\n",
    "test=train_df[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df['forecast'] = arima_mod6.predict(start = start_index, end= end_index, dynamic= True)  \n",
    "train_df[start_index:end_index][['sales', 'forecast']].plot(figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape_kun(y_true, y_pred):\n",
    "    mape = np.mean(abs((y_true-y_pred)/y_true))*100\n",
    "    #smape = np.mean((np.abs(y_pred - y_true) * 200/ (np.abs(y_pred) + np.abs(y_true))).fillna(0))\n",
    "    print('MAPE: %.2f'% (mape), \"%\")\n",
    "    \n",
    "#from sklearn.metrics import mean_squared_error\n",
    "#rmse = np.sqrt(mean_squared_error(test['sales'], train_df['forecast'])).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_kun(train_df[1730:1825]['sales'],train_df[1730:1825]['forecast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import normaltest\n",
    "\n",
    "resid = arima_mod6.resid\n",
    "print(normaltest(resid))\n",
    "# returns a 2-tuple of the chi-squared statistic, and the associated p-value. the p-value is very small, meaning\n",
    "# the residual is not a normal distribution\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax0 = fig.add_subplot(111)\n",
    "\n",
    "sns.distplot(resid ,fit = stats.norm, ax = ax0) # need to import scipy.stats\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = stats.norm.fit(resid)\n",
    "\n",
    "#Now plot the distribution using \n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual distribution')\n",
    "\n",
    "\n",
    "# ACF and PACF\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "fig = sm.graphics.tsa.plot_acf(arima_mod6.resid, lags=40, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "fig = sm.graphics.tsa.plot_pacf(arima_mod6.resid, lags=40, ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarima_mod6 = sm.tsa.statespace.SARIMAX(train_df.sales, trend='n', order=(14,1,1), seasonalorder=(14,1,7)).fit()\n",
    "print(sarima_mod6.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid = sarima_mod6.resid\n",
    "print(normaltest(resid))\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax0 = fig.add_subplot(111)\n",
    "\n",
    "sns.distplot(resid ,fit = stats.norm, ax = ax0) # need to import scipy.stats\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = stats.norm.fit(resid)\n",
    "\n",
    "#Now plot the distribution using \n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual distribution')\n",
    "\n",
    "# ACF and PACF\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "fig = sm.graphics.tsa.plot_acf(arima_mod6.resid, lags=40, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "fig = sm.graphics.tsa.plot_pacf(arima_mod6.resid, lags=40, ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 1730\n",
    "end_index = 1826\n",
    "train_df['forecast'] = sarima_mod6.predict(start = start_index, end= end_index, dynamic= True)  \n",
    "train_df[start_index:end_index][['sales', 'forecast']].plot(figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape_kun(y_true, y_pred):\n",
    "    mape = np.mean(abs((y_true-y_pred)/y_true))*100\n",
    "    #smape = np.mean((np.abs(y_pred - y_true) * 200/ (np.abs(y_pred) + np.abs(y_true))).fillna(0))\n",
    "    print('MAPE: %.2f'% (mape), \"%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_kun(train_df[1730:1825]['sales'],train_df[1730:1825]['forecast'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holt Winters' additive method with trend and seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "y_hat_hwa = train_df[start_index:end_index]\n",
    "model = ExponentialSmoothing(np.asarray(train_df['sales']), seasonal_periods=12, trend='add',seasonal='add')\n",
    "model_fit=model.fit(optimized=True)\n",
    "print(model_fit.params)\n",
    "y_hat_hwa['hw_forecast']=model_fit.forecast(96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=train_df[start_index:end_index]\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(train_df['sales'], label='Train')\n",
    "plt.plot(test['sales'], label='Test')\n",
    "plt.plot(y_hat_hwa['hw_forecast'],label='Holt Winters\\'s additive forecast')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Holt Winters\\'s Additive Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "rmse = np.sqrt(mean_squared_error(test['sales'], y_hat_hwa['hw_forecast'])).round(2)\n",
    "mape = np.round(np.mean(np.abs(test['sales']-y_hat_hwa['hw_forecast'])/test['sales'])*100,2)\n",
    "\n",
    "tempresults=pd.DataFrame({'Method':['Holt Winters\\'s Additive Method'], 'RMSE':[rmse], 'MAPE':[mape]})\n",
    "#results=pd.concat([results, tempresults])\n",
    "#results=results[['Method','RMSE','MAPE']]\n",
    "tempresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dynamic harmonic regression for multiple seasinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## vector regression (cross correlation bet y, x and y and x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
